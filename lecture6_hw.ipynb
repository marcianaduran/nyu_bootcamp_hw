{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name:** Claudia Duran Garcia \\\n",
    "**Email:** cmd9763@nyu.edu\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train/test/split strategy consists of splitting the dataset into three parts:\n",
    "# a training set, a validation set, and a test set. The training set is used to train the model,\n",
    "# the validation set is used to tune hyperparameters and make decisions about the model, \n",
    "# and the test set is used to evaluate the final performance of the model.\n",
    "\n",
    "# Number of recordings in the dataset: 200 recordings * 44 phones * 100 people * 5 days = 4,400,000 recordings\n",
    "\n",
    "# In this example, we will use a 70/15/15 split for training, validation, and testing respectively.\n",
    "# For Kilian, we will use a 60/20/20 split to ensure enough data for validation and testing because\n",
    "# it has more recordings for a single person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9dcc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Dropdown\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be88a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of k on decision boundary\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(k_value=3):\n",
    "    # Create mesh\n",
    "    h = 0.2\n",
    "    x_min, x_max = -0.5, 10.5\n",
    "    y_min, y_max = -0.5, 10.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Train k-NN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_value)\n",
    "    knn.fit(X, y)\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "               s=100, edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('X bound', fontsize=13)\n",
    "    plt.ylabel('Y bound', fontsize=13)\n",
    "    plt.title(f'k-NN Decision Boundary (k={k_value})', fontsize=15, fontweight='bold')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    # Add legend\n",
    "    plt.scatter([], [], c='red', s=100, label='Negative Region', alpha=0.4)\n",
    "    plt.scatter([], [], c='blue', s=100, label='Positive Region', alpha=0.4)\n",
    "    plt.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if k_value == 1:\n",
    "        print(\"  k=1: Very jagged boundary - overfitting to noise!\")\n",
    "    elif k_value > 10:\n",
    "        print(\"  Large k: Very smooth boundary - might be too simple!\")\n",
    "    else:\n",
    "        print(\" Good k value: Balanced decision boundary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8050c9d50b1c4d1488943f9a7740b10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='k:', max=15, min=1), Output()), _dom_classes=('widget-in‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_decision_boundary(k_value=3)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-NN Classification\n",
    "X = np.array([[1, 2],\n",
    "              [1, 4],\n",
    "              [5, 4],\n",
    "              [3, 1],\n",
    "              [3, 2]])\n",
    "# Labels: 1 = Positive, 0 = Negative\n",
    "y = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "interact(plot_decision_boundary,\n",
    "         k_value=IntSlider(min=1, max=15, step=1, value=3, description='k:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfce421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling: prediction = 1\n",
      "Feature mins: [100.   1.]\n",
      "Feature maxs: [500.   4.]\n",
      "Query scaled: [[1. 0.]]\n",
      "After MinMax scaling: prediction = 0\n",
      "Manual scaled (should match): [[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "X_big = np.array([[100, 2],\n",
    "                  [100, 4],\n",
    "                  [500, 4],\n",
    "                  [300, 1],\n",
    "                  [300, 2]])\n",
    "y_big = np.array([1, 1, 1, 0, 0])  # 1 = Positive, 0 = Negative\n",
    "\n",
    "query = np.array([[500, 1]])  # point to classify\n",
    "\n",
    "# 1-NN before scaling\n",
    "knn_raw = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_raw.fit(X_big, y_big)\n",
    "pred_raw = knn_raw.predict(query)\n",
    "print(\"Before scaling: prediction =\", int(pred_raw[0]))  # 1 or 0\n",
    "\n",
    "# Scale each feature to [0, 1] using MinMax scaling (fit on training data)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_big)\n",
    "query_scaled = scaler.transform(query)\n",
    "\n",
    "print(\"Feature mins:\", scaler.data_min_)\n",
    "print(\"Feature maxs:\", scaler.data_max_)\n",
    "print(\"Query scaled:\", query_scaled)\n",
    "\n",
    "# 1-NN after scaling\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_scaled.fit(X_scaled, y_big)\n",
    "pred_scaled = knn_scaled.predict(query_scaled)\n",
    "print(\"After MinMax scaling: prediction =\", int(pred_scaled[0]))\n",
    "\n",
    "# Manual min-max formula (for reference)\n",
    "# x_scaled = (x - min) / (max - min)\n",
    "mins = scaler.data_min_\n",
    "maxs = scaler.data_max_\n",
    "manual_scaled = (query - mins) / (maxs - mins)\n",
    "print(\"Manual scaled (should match):\", manual_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65678332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Handling Missing Values: How can you modify K-NN to handle missing features in a test point?\n",
    "# You can modify K-NN to handle missing features by using distance metrics that can ignore missing values,\n",
    "# such as using only the available features to compute distances.\n",
    "\n",
    "# 4. High-dimensional Data: Why can K-NN still work well for images even with thousands of pixels?\n",
    "# You can use dimensionality reduction techniques (like PCA) to reduce the number of features while retaining\n",
    "# most of the variance in the data, making K-NN more effective in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "# 1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "# No, evaluating $h(x) = sign(w . x)$ for every $(x, y)$ in D_TR and D_TE will give you the predictions for both sets, but it does not directly provide information about the error rates. \n",
    "# To determine whether the test error is higher than the training error, you would need to compare the predicted labels to the actual labels in both datasets and calculate the error rates \n",
    "# (i.e., the proportion of misclassified examples) for each set.\n",
    "\n",
    "# 2. Why is there no need to compute training error explicitly for the Perceptron algorithm?\n",
    "# The Perceptron algorithm updates its weights based on misclassified training examples during the training process.\n",
    "# Therefore, if the training process has converged, it implies that the training error is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc3c082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron class defined!\n"
     ]
    }
   ],
   "source": [
    "# Perceptron implementation\n",
    "# y = mx + c -> y = wx + b\n",
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.history = []\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.sign(np.dot(self.w, x) + self.b)\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        if self.w is None:\n",
    "            self.w = np.zeros(len(x))\n",
    "        \n",
    "        prediction = self.predict(x)\n",
    "        \n",
    "        # Save state before update\n",
    "        self.history.append({\n",
    "            'w': self.w.copy(),\n",
    "            'b': self.b,\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': prediction,\n",
    "            'correct': prediction == y\n",
    "        })\n",
    "        \n",
    "        # Update if wrong\n",
    "        if prediction != y:\n",
    "            self.w += y * x\n",
    "            self.b += y\n",
    "            return False  # Made an update\n",
    "        return True  # Correct prediction\n",
    "\n",
    "print(\"Perceptron class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated linearly separable dataset:\n",
      "   - 10 positive examples\n",
      "   - 12 negative examples\n"
     ]
    }
   ],
   "source": [
    "# Todo\n",
    "# Generate linearly separable data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Positive class\n",
    "X_pos = np.random.randn(10, 2) + np.array([-2, -2])\n",
    "y_pos = np.ones(20)\n",
    "\n",
    "# Negative class\n",
    "X_neg = np.random.randn(12, 2) + np.array([2, 2])\n",
    "y_neg = -np.ones(20)\n",
    "\n",
    "# Combine\n",
    "X_perceptron = np.vstack([X_pos, X_neg])\n",
    "y_perceptron = np.concatenate([y_pos, y_neg])\n",
    "\n",
    "print(f\"‚úÖ Generated linearly separable dataset:\")\n",
    "print(f\"   - {len(X_pos)} positive examples\")\n",
    "print(f\"   - {len(X_neg)} negative examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6027bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Training Perceptron...\n",
      "\n",
      "============================================================\n",
      "Epoch 1: 3 errors, w=[-0.77610366 -2.53540888], b=1.00\n",
      "Epoch 2: 2 errors, w=[-0.04892147 -2.93255346], b=1.00\n",
      "Epoch 3: 2 errors, w=[ 0.67826072 -3.32969805], b=1.00\n",
      "Epoch 4: 2 errors, w=[ 1.40544291 -3.72684263], b=1.00\n",
      "Epoch 5: 2 errors, w=[ 2.1326251  -4.12398721], b=1.00\n",
      "Epoch 6: 4 errors, w=[ 0.36371746 -2.03105102], b=1.00\n",
      "Epoch 7: 2 errors, w=[ 1.09089965 -2.4281956 ], b=1.00\n",
      "Epoch 8: 2 errors, w=[-0.19194965 -2.48864129], b=1.00\n",
      "Epoch 9: 2 errors, w=[ 0.53523254 -2.88578588], b=1.00\n",
      "Epoch 10: 2 errors, w=[ 1.26241473 -3.28293046], b=1.00\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train perceptron and visualize\n",
    "perceptron = Perceptron()\n",
    "\n",
    "print(\"üéì Training Perceptron...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_epochs = 10\n",
    "for epoch in range(max_epochs):\n",
    "    errors = 0\n",
    "    for i, (x, y) in enumerate(zip(X_perceptron, y_perceptron)):\n",
    "        correct = perceptron.train_step(x, y)\n",
    "        if not correct:\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: {errors} errors, w={perceptron.w}, b={perceptron.b:.2f}\")\n",
    "    \n",
    "    if errors == 0:\n",
    "        print(f\"\\nüéâ Converged after {epoch + 1} epochs!\")\n",
    "        break\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88387ed1d5d24d9f8961daac5654f2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='Step:', max=219), Output()), _d‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_perceptron_step(step=0)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive perceptron visualization\n",
    "def visualize_perceptron_step(step=0):\n",
    "    if step >= len(perceptron.history):\n",
    "        step = len(perceptron.history) - 1\n",
    "    \n",
    "    state = perceptron.history[step]\n",
    "    w = state['w']\n",
    "    b = state['b']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X_pos[:, 0], X_pos[:, 1], c='blue', s=100, alpha=0.6, \n",
    "               label='Positive (+1)', edgecolors='black')\n",
    "    plt.scatter(X_neg[:, 0], X_neg[:, 1], c='red', s=100, alpha=0.6, \n",
    "               label='Negative (-1)', edgecolors='black')\n",
    "    \n",
    "    # Highlight current point\n",
    "    current_x = state['x']\n",
    "    current_y = state['y']\n",
    "    color = 'green' if state['correct'] else 'orange'\n",
    "    plt.scatter(current_x[0], current_x[1], c=color, s=400, marker='*',\n",
    "               edgecolors='black', linewidths=2, zorder=5,\n",
    "               label='Current Point')\n",
    "    \n",
    "    # Plot decision boundary if weights are non-zero\n",
    "    if np.any(w != 0):\n",
    "        x_line = np.linspace(-5, 5, 100)\n",
    "        # w1*x1 + w2*x2 + b = 0  =>  x2 = -(w1*x1 + b)/w2\n",
    "        if w[1] != 0:\n",
    "            y_line = -(w[0] * x_line + b) / w[1]\n",
    "            plt.plot(x_line, y_line, 'g-', linewidth=3, label='Decision Boundary')\n",
    "            \n",
    "            # Plot normal vector (direction of w)\n",
    "            origin = np.array([0, -b/w[1]]) if w[1] != 0 else np.array([-b/w[0], 0])\n",
    "            plt.arrow(origin[0], origin[1], w[0]*0.5, w[1]*0.5, \n",
    "                     head_width=0.3, head_length=0.2, fc='purple', ec='purple',\n",
    "                     linewidth=2, label='Weight Vector')\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=13)\n",
    "    plt.ylabel('Feature 2', fontsize=13)\n",
    "    plt.title(f'Perceptron Learning - Step {step + 1}/{len(perceptron.history)}\\n' +\n",
    "             f'w=[{w[0]:.2f}, {w[1]:.2f}], b={b:.2f}\\n' +\n",
    "             f'Prediction: {int(state[\"pred\"])}, True: {int(current_y)} - ' +\n",
    "             ('‚úì Correct' if state['correct'] else '‚úó Wrong, Updating!'),\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=10, loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìç Step {step + 1} Details:\")\n",
    "    print(f\"   Current point: x = {current_x}\")\n",
    "    print(f\"   True label: y = {int(current_y)}\")\n",
    "    print(f\"   Prediction: ≈∑ = {int(state['pred'])}\")\n",
    "    print(f\"   Score: w¬∑x + b = {np.dot(w, current_x) + b:.2f}\")\n",
    "    \n",
    "    if not state['correct']:\n",
    "        new_w = w + current_y * current_x\n",
    "        new_b = b + current_y\n",
    "        print(f\"   ‚ùå Wrong prediction! Updating...\")\n",
    "        print(f\"   New w: {w} + {current_y} √ó {current_x} = {new_w}\")\n",
    "        print(f\"   New b: {b:.2f} + {current_y} = {new_b:.2f}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Correct! No update needed.\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(visualize_perceptron_step,\n",
    "         step=IntSlider(min=0, max=len(perceptron.history)-1, step=1, value=0,\n",
    "                       description='Step:', continuous_update=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a1984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
